{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Chatbot RAG con Memoria para E-commerce  ‚Äì por Mauricio Rostagno\n",
        "\n",
        "Este proyecto es un prototipo funcional de **chatbot inteligente** desarrollado como parte de mi portafolio de aplicaciones con inteligencia artificial y lenguaje natural. Uso a modo de simulaci√≥n la necesidad de asistir a clientas de un emprendimiento online, pero puede adaptarse f√°cilmente a cualquier comercio que quiera ofrecer atenci√≥n automatizada por redes o web.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Tecnolog√≠as utilizadas\n",
        "\n",
        "- **LangChain**: Framework modular para construir agentes y pipelines con LLMs.\n",
        "- **ChromaDB**: Vector store local para almacenar embeddings y realizar b√∫squedas sem√°nticas.\n",
        "- **HuggingFace Transformers**: Para usar modelos como `flan-t5-large` (entrenado para tareas de texto).\n",
        "- **Groq + LLaMA3**: Modelo de lenguaje potente, r√°pido y gratuito para generar respuestas naturales.\n",
        "- **Python + Google Colab**: Desarrollo y ejecuci√≥n del prototipo.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© ¬øQu√© hace este chatbot?\n",
        "\n",
        "- Responde preguntas sobre el negocio, productos y formas de pago.\n",
        "- Recupera contexto relevante con un pipeline **RAG (Retrieval-Augmented Generation)**.\n",
        "- Usa **memoria conversacional** para mantener coherencia en el di√°logo.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Pr√≥ximos pasos en los que trabajo\n",
        "\n",
        "- Integraci√≥n con Google Sheets para leer productos actualizados.\n",
        "- Montaje de una interfaz visual (ej. Streamlit o Gradio).\n",
        "- Posible despliegue como asistente en redes sociales o WhatsApp Business.\n",
        "\n",
        "---\n",
        "\n",
        "üì¨ Contacto: rostagno.mj@gmail.com  \n",
        "üîó GitHub: [@MauriRos](https://github.com/MauriRos)  \n",
        "üåê LinkedIn: [linkedin.com/in/mauricio-rostagno](https://www.linkedin.com/in/mauricio-rostagno)\n"
      ],
      "metadata": {
        "id": "Id0vCvtlKo8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Paso 0: Instalaci√≥n de librer√≠as necesarias\n",
        "\n",
        "En esta celda instalamos todas las dependencias para construir nuestro chatbot RAG con memoria.\n",
        "\n",
        "Incluye:\n",
        "- `LangChain`: framework para aplicaciones con LLMs.\n",
        "- `ChromaDB` y `FAISS`: almacenamiento vectorial.\n",
        "- `Transformers`: modelos de lenguaje de Hugging Face.\n",
        "- `sentence-transformers`: modelos de embeddings.\n",
        "- `langchain-groq`: para conectarnos con el modelo LLaMA 3 desde la API de Groq.\n"
      ],
      "metadata": {
        "id": "2QNEd4xz-Tzo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I9RxmcJPKnqa"
      },
      "outputs": [],
      "source": [
        "# Instalamos las librer√≠as necesarias\n",
        "!pip install langchain langchain-community langchain-core\n",
        "!pip install openai chromadb faiss-cpu tiktoken\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install langchain-groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Paso 1: Importaci√≥n de librer√≠as\n",
        "\n",
        "En este bloque importamos todas las librer√≠as que usaremos en el proyecto, tanto para manejo de datos como para crear la l√≥gica del chatbot.\n",
        "\n",
        "Incluye:\n",
        "- Librer√≠as de Python (`numpy`, `pandas`)\n",
        "- Componentes clave de `LangChain`\n",
        "- El pipeline de modelos de `transformers`\n"
      ],
      "metadata": {
        "id": "AYTR6nxc-Jda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Librer√≠as b√°sicas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# LangChain\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma, FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Transformers\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "i29mpeeeKxYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõçÔ∏è Paso 2: Definir la informaci√≥n del negocio\n",
        "\n",
        "Aqu√≠ cargamos los textos que contienen la informaci√≥n real de Via√©Bags Marroquiner√≠a: qu√© productos vende, c√≥mo se realizan los pagos y env√≠os, formas de contacto, etc.\n",
        "\n",
        "Estos textos ser√°n utilizados como base de conocimiento del asistente virtual.\n"
      ],
      "metadata": {
        "id": "zf9nOhxd-fv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_texts = [\n",
        "   \"Via√©Bags Marroquiner√≠a es un emprendimiento de Santa Fe que vende carteras, bolsos, mochilas y equipaje por Instagram.\",\n",
        "    \"No contamos con showroom ni local f√≠sico. Solo ofrecemos retiro por domicilio particular o env√≠o a todo el pa√≠s.\",\n",
        "    \"Realizamos env√≠os a cualquier parte de Argentina mediante servicios de correo o retiro por domicilio. No hay atenci√≥n presencial.\",\n",
        "    \"Pod√©s retirar por un punto de la ciudad de Santa Fe si est√°s cerca. Coordinamos el punto exacto por mensaje.\",\n",
        "    \"Aceptamos todos los medios de pago: tarjetas de cr√©dito, d√©bito, transferencias y Mercado Pago.\",\n",
        "    \"Consultanos por descuentos en efectivo o promociones especiales vigentes.\",\n",
        "    \"Vendemos varias marcas reconocidas y destacamos por tener los mejores precios del mercado.\",\n",
        "    \"Frecuentemente realizamos sorteos y promociones en nuestra cuenta de Instagram: @viae.bags.\",\n",
        "    \"Atendemos consultas por Instagram y por correo electr√≥nico: viabagsonline@gmail.com.\",\n",
        "    \"Respondemos con amabilidad, claridad y cercan√≠a todas las dudas sobre productos, precios, medios de pago y disponibilidad.\",\n",
        "    \"Pod√©s consultarnos por cat√°logo de productos, disponibilidad de stock o promociones activas.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "kNwf3WsULray"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÑ Paso 3: Convertir textos en objetos `Document`\n",
        "\n",
        "Los textos cargados se convierten en objetos `Document`, un formato compatible con LangChain que permite luego procesarlos, dividirlos y vectorizarlos para el sistema de recuperaci√≥n de informaci√≥n.\n",
        "\n",
        "Esto nos permite que cada fragmento tenga estructura y pueda usarse dentro del flujo RAG.\n"
      ],
      "metadata": {
        "id": "9BZRPbOK_AJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "# Convertimos los textos en objetos Document\n",
        "documents = [Document(page_content=text) for text in sample_texts]\n"
      ],
      "metadata": {
        "id": "bUCEmSSdL649"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÇÔ∏è Paso 4: Dividir textos en fragmentos (chunks)\n",
        "\n",
        "Dividimos cada documento en fragmentos de m√°ximo 200 caracteres, con un solapamiento de 50. Esto mejora la recuperaci√≥n posterior, ya que permite encontrar secciones m√°s relevantes dentro del contexto original.\n",
        "\n",
        "LangChain se encarga autom√°ticamente de cortar y preservar la coherencia del texto.\n"
      ],
      "metadata": {
        "id": "zdyLczWo_B6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Creamos el splitter (divisor de texto)\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=200,        # M√°ximo 200 caracteres por fragmento\n",
        "    chunk_overlap=50       # Solapamiento de 50 caracteres entre fragmentos\n",
        ")\n",
        "\n",
        "# Aplicamos el splitter a nuestros documentos\n",
        "chunked_documents = text_splitter.split_documents(documents)\n"
      ],
      "metadata": {
        "id": "DL2BdtBuRVP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Paso 5: Generar embeddings y crear la base vectorial\n",
        "\n",
        "En este paso transformamos los textos en vectores num√©ricos (embeddings) utilizando el modelo `all-MiniLM-L6-v2` de Hugging Face. Luego almacenamos esos vectores en una base vectorial llamada **ChromaDB**, que nos permitir√° buscar informaci√≥n relevante a partir de una consulta.\n",
        "\n",
        "üì¶ Esta base queda guardada en el disco (`./chroma_db`) para reutilizarla m√°s adelante.\n"
      ],
      "metadata": {
        "id": "_XolYGrqDhXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Creamos el modelo de embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "persist_dir = \"./chroma_db\"\n",
        "# Creamos la base vectorial con ChromaDB\n",
        "chroma_db = Chroma.from_documents(documents=chunked_documents, embedding=embedding_model, persist_directory=persist_dir)\n",
        "\n",
        "chroma_db.persist()\n"
      ],
      "metadata": {
        "id": "ayMqZICySVMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Paso 6: Conectar el modelo LLaMA 3 v√≠a Groq\n",
        "\n",
        "En esta celda conectamos nuestro chatbot a la API de **Groq**, que nos permite usar el modelo **LLaMA 3** de forma gratuita y extremadamente r√°pida.\n",
        "\n",
        "El modelo se invoca a trav√©s del componente `ChatGroq`, que se integra f√°cilmente con LangChain. Esta es la inteligencia central que va a generar las respuestas.\n"
      ],
      "metadata": {
        "id": "vePY9-_wDlkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Requiere instalar groq primero (ya lo hiciste, si no: !pip install groq)\n",
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "\n",
        "# Peg√° tu Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"  # ‚Üê reemplaz√° esto por tu key real\n",
        "\n",
        "# Crear el LLM de Groq usando LLaMA3\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_name=\"llama3-8b-8192\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "FPRdSmrMTZ2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Paso 7: Crear el prompt, el retriever y el pipeline RAG\n",
        "\n",
        "En este bloque definimos tres componentes clave:\n",
        "\n",
        "- **Prompt personalizado:** Le da instrucciones al modelo sobre c√≥mo debe responder, incluyendo el tono y el contexto.\n",
        "- **Retriever:** Recupera fragmentos relevantes desde la base Chroma a partir de la consulta del usuario.\n",
        "- **Pipeline RAG (Retrieval-Augmented Generation):** Une todo en una cadena secuencial: recibe una pregunta, recupera contexto, construye el prompt y genera una respuesta.\n",
        "\n",
        "Adem√°s, agregamos un paso intermedio para **mostrar el prompt por consola**, ideal para debugging y entender qu√© est√° viendo el modelo.\n"
      ],
      "metadata": {
        "id": "PkcQbXu1Dqvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# 1. Crear un prompt template (como antes)\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "Sos un asistente virtual de Via√©Bags Marroquiner√≠a, una tienda de carteras y bolsos.\n",
        "Respond√© con claridad y solo usando esta informaci√≥n:\n",
        "\n",
        "{context}\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Respuesta:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Esta funci√≥n une el contexto y la pregunta en un string prompt listo para el modelo\n",
        "build_prompt = RunnableLambda(lambda x: prompt_template.format(**x))\n",
        "\n",
        "# 2. Crear el retriever\n",
        "retriever = chroma_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "\n",
        "\n",
        "# Funci√≥n que extrae la pregunta del input completo\n",
        "#extract_question = RunnableLambda(lambda x: x[\"question\"])\n",
        "\n",
        "# Procesar el contexto para convertir documentos a texto plano\n",
        "def extract_context(docs):\n",
        "    if isinstance(docs, list):\n",
        "        return \"\\n\".join([doc.page_content for doc in docs])\n",
        "    return docs\n",
        "\n",
        "extract_context_lambda = RunnableLambda(extract_context)\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Esto te muestra por pantalla el prompt que se le env√≠a al modelo\n",
        "debug_prompt = RunnableLambda(lambda x: print(\"üì§ Prompt al modelo:\\n\", x) or x)\n",
        "\n",
        "# Creamos la nueva cadena con filtro\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": extract_question | retriever | extract_context_lambda,\n",
        "        \"question\": extract_question\n",
        "    }\n",
        "    | build_prompt\n",
        "    #| debug_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ],
      "metadata": {
        "id": "isfbLkFmYMix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Paso 8: Incorporar memoria conversacional\n",
        "\n",
        "Este paso nos permite que el chatbot recuerde los mensajes anteriores de cada usuario. As√≠ se logra una conversaci√≥n m√°s coherente, donde el modelo puede responder de forma contextual.\n",
        "\n",
        "üîÅ Usamos `RunnableWithMessageHistory` para conectar el pipeline RAG con un historial de chat en memoria. Esto asocia cada sesi√≥n de usuario (por ejemplo: `\"cliente123\"`) con su conversaci√≥n previa.\n",
        "\n",
        "Finalmente, hacemos una primera prueba enviando la pregunta: `\"¬øPuedo pagar con tarjeta?\"`.\n"
      ],
      "metadata": {
        "id": "U1viQ-FOTVtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory.chat_message_histories.in_memory import ChatMessageHistory\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory.chat_message_histories.in_memory import ChatMessageHistory\n",
        "\n",
        "def get_message_history(session_id: str):\n",
        "    return ChatMessageHistory()\n",
        "rag_with_memory = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_message_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n",
        "response = rag_with_memory.invoke(\n",
        "    {\"question\": \"¬øPuedo pagar con tarjeta?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"cliente123\"}}\n",
        ")\n",
        "print(\"ü§ñ\", response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egp6spRqkjNt",
        "outputId": "9237974b-2193-4fa1-bc20-2a352fa4b7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ ¬°Claro! En Via√©Bags Marroquiner√≠a, aceptamos pagos con tarjeta. Podr√°s elegir entre varias opciones de pago seguras y convenientes para ti. ¬°No hay mejor manera de disfrutar de nuestros productos reconocidos y de nuestras ofertas exclusivas que con la comodidad de pagar con tarjeta!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_with_memory.invoke(\n",
        "    {\"question\": \"¬øQu√© productos venden?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"cliente1\"}}\n",
        ")\n",
        "print(\"ü§ñ\", response)\n",
        "\n",
        "response = rag_with_memory.invoke(\n",
        "    {\"question\": \"¬øPuedo pagar con tarjeta?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"cliente1\"}}\n",
        ")\n",
        "print(\"ü§ñ\", response)\n",
        "\n",
        "response = rag_with_memory.invoke(\n",
        "    {\"question\": \"¬øY si quiero devolverlo?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"cliente1\"}}\n",
        ")\n",
        "print(\"ü§ñ\", response)\n",
        "\n",
        "response = rag_with_memory.invoke(\n",
        "    {\"question\": \"¬øD√≥nde se puede retirar?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"cliente1\"}}\n",
        ")\n",
        "print(\"ü§ñ\", response)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNB7ceS-YQHJ",
        "outputId": "8ff92793-b3ea-4e21-91b4-a31ed4a55bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ ¬°Hola! En Via√©Bags Marroquiner√≠a, vendemos una amplia variedad de productos relacionados con carteras y bolsos. Ofrecemos una selecci√≥n de modelos de carteras para hombres y mujeres, desde peque√±as bolsas de mano hasta grandes bolsos de viaje. Tambi√©n tenemos una gran variedad de accesorios, como cinturones, corbata y otros complementos para completar tu look. ¬°Si necesitas algo en particular, no dudes en preguntar!\n",
            "ü§ñ ¬°Claro! En Via√©Bags Marroquiner√≠a, aceptamos pagos con tarjeta de cr√©dito o d√©bito, ofreciendo la mayor comodidad y seguridad para tus compras.\n",
            "ü§ñ Excelente elecci√≥n! En Via√©Bags Marroquiner√≠a, nos esforzamos por ofrecer productos de alta calidad y precios competitivos. Si deseas devolver un producto, no hay problema. Puedes hacerlo dentro de un plazo de [insertar plazo de devoluci√≥n] despu√©s de la fecha de compra, siempre y cuando el producto est√© en perfecto estado y tenga todos los elementos originales. Debes presentar la factura y el producto en nuestra tienda para iniciar el proceso de devoluci√≥n. ¬°No dudes en preguntar!\n",
            "ü§ñ ¬°Hola! En Via√©Bags Marroquiner√≠a, podemos enviar los productos directamente a tu domicilio, sin necesidad de retirarlos en tienda. Sin embargo, si deseas retirar tu compra en persona, podemos proporcionarte informaci√≥n sobre nuestras tiendas f√≠sicas. ¬øPodr√≠as decirme en qu√© ubicaci√≥n te gustar√≠a retirar el producto?\n"
          ]
        }
      ]
    }
  ]
}